<!DOCTYPE html>
<html lang="en-US">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta charset="UTF-8">

<meta property="og:image" content="favicon.png">

<link rel="icon" href="favicon.png" type="image/png">

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Recognizing Multimodal Entailment</title>
<meta name="generator" content="Jekyll v3.9.0">
<meta property="og:title" content="Welcome">
<meta property="og:locale" content="en_US">
<meta name="description" content="Recognizing Multimodal Entailment at ACL-IJCNLP 2021: The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing.">
<meta property="og:description" content="Recognizing Multimodal Entailment at ACL-IJCNLP 2021: The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing.">
<link rel="canonical" href="https://multimodal-entailment.github.io//">
<meta property="og:url" content="https://multimodal-entailment.github.io//">
<meta property="og:site_name" content="Recognizing Multimodal Entailment">
<script type="application/ld+json">
{"description":"Recognizing Multimodal Entailment at ACL-IJCNLP 2021: The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing.","name":"Recognizing Multimodal Entailment","@type":"WebSite","url":"https://multimodal-entailment.github.io//","headline":"Welcome","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=c64178984bc3b4827ae20ab3ebe9e05d036cc516">
  </head>
  <body>

    <header class="page-header" role="banner">
    <h1>Recognizing Multimodal Entailment</h1>
    <h2>ACL-IJCNLP 2021: The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</h2>
    </header>

    <main id="content" class="main-content" role="main">


<div style="display: flex, margin: auto" align="center">  
  <div style="width:64px;" align="center">
    <img src="favicon.png" style="max-width:100%;">
  </div>
  </div>

      <h2 id="welcome">Welcome</h2>

<p>How information is created, shared and consumed has changed rapidly in recent decades, in part thanks to new social platforms and technologies on the web. With ever-larger amounts of unstructured and limited labels, organizing and reconciling information from different sources and modalities is a central challenge in machine learning.
<br><br>
This cutting-edge tutorial aims to introduce the multimodal entailment task, which can be useful for detecting semantic alignments when a single modality alone does not suffice for a whole content understanding. Starting with a brief overview of natural language processing, computer vision, structured data and neural graph learning, we lay the foundations for the multimodal sections to follow. We then discuss recent multimodal learning literature covering visual, audio and language streams, and explore case studies focusing on tasks which require fine-grained understanding of visual and linguistic semantics question answering, veracity and hatred classification. Finally, we introduce a new dataset for recognizing multimodal entailment, exploring it in a hands-on collaborative section. 
<br><br>
Overall, this tutorial gives an overview of multimodal learning, introduces a multimodal entailment dataset, and encourages future research in the topic.
</p>

<h2 id="venue">Venue</h2>

<p>The <em>Recognizing Multimodal Entailment</em> tutorial will be held virtually at <a href="https://2021.aclweb.org/"><em>ACL-IJCNLP 2021: The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</em></a> on Sunday, August 1st, 2021.</p>

<h2 id="outline">Outline</h2>

<table>
   <tbody>
      <tr>
         <td>Section</td>
         <td>Subsection</td>
         <td style="text-align:right;">min</td>
      </tr>
      <tr>
         <td rowspan="2">Introduction
            <br>
         </td>
         <td>The landscape of online content</td>
         <td style="text-align:right;">5</td>
      </tr>
      <tr>
         <td>A case for multimodal entailment inferences</td>
         <td style="text-align:right;">5</td>
      </tr>
      <tr>
         <td rowspan="3">Natural
            <br>
            Language
            <br>
            Processing
         </td>
         <td>From word embeddings to contextualized representations</td>
         <td style="text-align:right;">15</td>
      </tr>
      <tr>
         <td>Fine-tuning pretrained models on downstream tasks</td>
         <td style="text-align:right;">5</td>
      </tr>
      <tr>
         <td>The textual entailment problem</td>
         <td style="text-align:right;">5</td>
      </tr>
      <tr>
         <td rowspan="2">Structured Data
            <br>
         </td>
         <td>Semi-structured and tabular text</td>
         <td style="text-align:right;">5</td>
      </tr>
      <tr>
         <td>Knowledge graphs</td>
         <td style="text-align:right;">5</td>
      </tr>
      <tr>
         <td>Neural Graph Learning</td>
         <td>Leveraging structured signals with Neural Structured Learning</td>
         <td style="text-align:right;">10</td>
      </tr>
      <tr>
         <td>Computer Vision</td>
         <td>Foundations of Computer Vision</td>
         <td style="text-align:right;">20</td>
      </tr>
      <tr>
         <td>Break</td>
         <td>-</td>
         <td style="text-align:right;">10</td>
      </tr>
      <tr>
         <td rowspan="3">Multimodal Learning
            <br><br>
         </td>
         <td>Attention Bottlenecks for Multimodal Fusion: state-of-the-art audio-visual classifications</td>
         <td style="text-align:right;">10</td>
      </tr>
      <tr>
         <td>Self-Supervised Multimodal Versatile Networks:  visual, audio and language streams</td>
         <td style="text-align:right;">20</td>
      </tr>
      <tr>
         <td>Case studies: cross-modal fine-grained reasoning</td>
         <td style="text-align:right;">20</td>
      </tr>
      <tr>
         <td>Break</td>
         <td>-</td>
         <td style="text-align:right;">10</td>
      </tr>
      <tr>
         <td rowspan="2">Multimodal entailment
            <br>
         </td>
         <td>Multimodal models for entailment inferences</td>
         <td style="text-align:right;">15</td>
      </tr>
      <tr>
         <td>Multimodal entailment dataset</td>
         <td style="text-align:right;">10</td>
      </tr>
      <tr>
         <td rowspan="2">Final considerations
            <br>
         </td>
         <td>Closing notes</td>
         <td style="text-align:right;">5</td>
      </tr>
      <tr>
         <td>Q&amp;A</td>
         <td style="text-align:right;">10</td>
      </tr>
      <tr>
         <td>Total</td>
         <td>–</td>
         <td style="text-align:right;">195</td>
      </tr>
   </tbody>
</table>


<h2 id="slides">Slides</h2>

<iframe
  src="https://docs.google.com/presentation/d/1mAB31BCmqzfedreNZYn4hsKPFmgHA9Kxz219DzyRY3c/embed?usp=sharing&resourcekey=0-ndOW4fXhhrY_cuvDU4Wcxw"
  frameborder="0"
  width="800"
  height="600"
></iframe>

<h2 id="reading-list">Reading list</h2>

<h3 id="natural-language-processing">Natural Language Processing</h3>

<ul>
<li><a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a>, Vaswani et at., 2017.</li>
<li><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>, Devlin et at., 2018.</li>
<li><a href="https://arxiv.org/abs/1706.03762">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>, Raffel et at., 2019.</li>
<li><a href="https://github.com/tensorflow/workshops/tree/master/kdd2019">Deep Learning for NLP with Tensorflow</a>, Ilharco et at., 2019.</li>
<li><a href="http://gabrielilharco.com/publications/EMNLP_2020_Tutorial__High_Performance_NLP.pdf">High Performance Natural Language Processing</a>, Ilharco et at., 2020.</li>
<li><a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a>, Brown et at., 2020.</li>
<li><a href="https://arxiv.org/abs/2006.16668">GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</a>, Lepikhin et at., 2020.</li>
<li><a href="https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/">DeepSpeed: Extreme-scale model training for everyone</a>, Majumder et at., 2020.</li>
</ul>

<h3 id="neural-graph-learning">Textual Entailment</h3>

<ul>
<li><a href="https://allenai.org/content/team/peterc/publications/RTE7_overview.proceedings.pdf">The Seventh PASCAL Recognizing Textual Entailment Challenge</a>, Bentivogli et at., 2011.</li>
<li><a href="https://www.mitpressjournals.org/doi/full/10.1162/COLI_a_00097">Did It Happen? The Pragmatic Complexity of Veridicality Assessment</a>, de Marneffe et at., 2012.</li>
<li><a href="https://cims.nyu.edu/~sbowman/multinli/">The Multi-Genre Natural Language Inference (MultiNLI) corpus</a>, Williams et at., 2017.</li>
<li><a href="https://arxiv.org/abs/1809.05053">XNLI: Evaluating Cross-lingual Sentence Representations</a>, Conneau et at., 2018.</li>
</ul>

<h3 id="neural-graph-learning">Structured Data</h3>

<ul>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/3329781.3332266">Industry-scale Knowledge Graphs</a>, Noy et at., 2019.</li>
<li><a href="https://www.meetup.com/NLP-Zurich/events/263974607/">Understanding categorical semantic compatibility in KG</a>, Muxagata et at., 2019.</li>
</ul>


<h3 id="neural-graph-learning">Neural Graph Learning</h3>

<ul>
<li><a href="https://arxiv.org/abs/1703.04818">Neural Graph Machines: Learning Neural Networks Using Graphs</a>, Bui et at., 2017.</li>
<li><a href="https://github.com/tensorflow/neural-structured-learning/tree/master/workshops/kdd_2020">Neural Structured Learning: Training Neural Networks with Structured Signals</a>, Heydon et at., 2020.</li>
</ul>

<h3 id="multimodal-learning">Multimodal Learning</h3>

<ul>
<li><a href="https://people.csail.mit.edu/khosla/papers/icml2011_ngiam.pdf">Multimodal Deep Learning</a>, Ngiam et at., 2011.</li>
<li><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41473.pdf">DeViSE: A Deep Visual-Semantic Embedding Model</a>, Frome et at., 2013.</li>
<li><a href="https://arxiv.org/abs/1804.02516">Learning a Text-Video Embedding from Incomplete and Heterogeneous Data</a>, Miech et at., 2018.</li>
<li><a href="https://arxiv.org/abs/1904.01766">VideoBERT: A Joint Model for Video and Language Representation Learning</a>, Sun et at., 2019.</li>
<li><a href="https://arxiv.org/abs/1906.03327">HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips</a>, Miech et at., 2019.</li>
<li><a href="https://arxiv.org/abs/1906.05743">Learning Video Representations using Contrastive Bidirectional Transformer</a>, Sun et at., 2019.</li>
<li><a href="https://arxiv.org/abs/1907.13487">Use What You Have: Video Retrieval Using Representations From Collaborative Experts</a>, Liu et at., 2019.</li>
<li><a href="https://arxiv.org/abs/1908.04472">Exploiting Multi-domain Visual Information for Fake News Detection</a>, Qi et at., 2019.</li>
<li><a href="https://arxiv.org/abs/1908.07490">LXMERT: Learning Cross-Modality Encoder Representations from Transformers</a>, Tan et at., 2019.</li>
<li><a href="https://arxiv.org/abs/1910.03814">Exploring Hate Speech Detection in Multimodal Publications</a>, Gomez et at., 2019.</li>
<li><a href="https://arxiv.org/abs/1908.08530">VL-BERT: Pre-training of Generic Visual-Linguistic Representations</a>, Su et at., 2020.</li>
<li><a href="https://arxiv.org/abs/1911.03854">r/Fakeddit: A New Multimodal Benchmark Dataset for Fine-grained Fake News Detection</a>, Nakamura et at., 2020.</li>
<li><a href="https://arxiv.org/abs/1912.02315">12-in-1: Multi-Task Vision and Language Representation Learning</a>, Lu et at., 2020.</li>
<li><a href="https://arxiv.org/abs/2003.04981">SAFE: Similarity-Aware Multi-Modal Fake News Detection</a>, Zhou et at., 2020.</li>
<li><a href="https://ieeexplore.ieee.org/document/9260096">Multimodal Multi-image Fake News Detection</a>, Giachanou et at., 2020.</li>
<li><a href="https://arxiv.org/abs/2003.13594">Speech2Action: Cross-modal Supervision for Action Recognition</a>, Nagrani et at., 2020.</li>
<li><a href="https://arxiv.org/abs/2004.04917">Multimodal Categorization of Crisis Events in Social Media</a>, Abavisani et at., 2020.</li>
<li><a href="https://arxiv.org/abs/2005.06035">Cross-Modality Relevance for Reasoning on Language and Vision</a>, Zheng et at., 2020.</li>
<li><a href="https://arxiv.org/abs/2006.16228">Self-Supervised MultiModal Versatile Networks</a>, Alayrac et at., 2020.</li>
<li><a href="https://arxiv.org/abs/2007.10639">Multi-modal Transformer for Video Retrieval</a>, Gabeur et at., 2020.</li>
<li><a href="https://arxiv.org/abs/2009.11278">X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers</a>, Cho et at., 2020.</li>
<li><a href="https://arxiv.org/abs/2005.04790">The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes</a>, Kiela et at., 2021.</li>
<li><a href="https://arxiv.org/abs/2101.06278">COSMOS: Catching Out-of-Context Misinformation with Self-Supervised Learning</a>, Aneja et at., 2021.</li>
<li><a href="https://arxiv.org/abs/2101.00529">VinVL: Revisiting Visual Representations in Vision-Language Models</a>, Zhang et at., 2021.</li>
<li><a href="https://arxiv.org/abs/2102.03334">ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</a>, Kim et at., 2021.</li>
<li><a href="https://arxiv.org/abs/2102.15409">UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning</a>, Li et at., 2021.</li>
<li><a href="https://arxiv.org/abs/2104.06039">MultiModalQA: Complex Question Answering over Text, Tables and Images</a>, Talmor et at., 2021.</li>
<li><a href="https://arxiv.org/abs/2107.00135">Attention Bottlenecks for Multimodal Fusion</a>, Nagrani et at., 2021.</li>
</ul>


<h2 id="tutors">Presenters</h2>

<div style="display: flex">
  <div style="width:250px;" align="center">
    <img alt="Afsaneh Shirazi" src="./images/afsaneh_shirazi.jpg" style="max-width:100%;">
    </a><br>
    Afsaneh Shirazi,<br>Google
  </div>
  
  <div style="width:2%">
  </div>
  
  <div style="width:250px;" align="center">
    <img alt="Arjun Gopalan" src="./images/arjun_gopalan.jpg" style="max-width:100%;">
    </a><br>
    Arjun Gopalan,<br>Google Research
  </div>

  <div style="width:2%">
  </div>

  <div style="width:250px;" align="center">
    <img alt="Arsha Nagrani" src="./images/arsha_nagrani.jpg" style="max-width:100%;">
    </a><br>
    Arsha Nagrani,<br>Google Research
  </div>
</div>

<br>

<div style="display: flex">
  <div style="width:250px;" align="center">
    <img alt="Cesar Ilharco" src="./images/cesar_ilharco.jpg" style="max-width:100%;">
    </a><br>
    Cesar Ilharco,<br>Google
  </div>
  
  <div style="width:2%">
  </div>
  
  <div style="width:250px;" align="center">
    <img alt="Christina Liu" src="./images/christina_liu.jpg" style="max-width:100%;">
    </a><br>
    Christina Liu,<br>Google Research
  </div>

  <div style="width:2%">
  </div>

  <div style="width:250px;" align="center">
    <img alt="Gabriel Barcik" src="./images/gabriel_barcik.jpg" style="max-width:100%;">
    </a><br>
    Gabriel Barcik,<br>Google Research
  </div>
</div>

<br>

<div style="display: flex">
  <div style="width:250px;" align="center">
    <img alt="Jannis Bulian" src="./images/jannis_bulian.jpg" style="max-width:100%;">
    </a><br>
    Jannis Bulian,<br>Google Research
  </div>
  
  <div style="width:2%">
  </div>
  
  <div style="width:250px;" align="center">
    <img alt="Jared Frank" src="./images/jared_frank.jpg" style="max-width:100%;">
    </a><br>
    Jared Frank,<br>Google
  </div>

  <div style="width:2%">
  </div>

  <div style="width:250px;" align="center">
    <img alt="Lucas Smaira" src="./images/lucas_smaira.jpg" style="max-width:100%;">
    </a><br>
    Lucas Smaira,<br>DeepMind
  </div>
</div>
<br>

<div style="display: flex">
  <div style="width:250px;" align="center">
    <img alt="Qin Cao" src="./images/qin_cao.jpg" style="max-width:100%;">
    </a><br>
    Qin Cao,<br>Google Research
  </div>
  
  <div style="width:2%">
  </div>
  
  <div style="width:250px;" align="center">
    <img alt="Ricardo Marino" src="./images/ricardo_marino.jpg" style="max-width:100%;">
    </a><br>
    Ricardo Marino,<br>Google
  </div>

  <div style="width:2%">
  </div>

  <div style="width:250px;" align="center">
    <img alt="Roma Patel" src="./images/roma_patel.jpg" style="max-width:100%;">
    </a><br>
    Roma Patel,<br>Brown University
  </div>
</div>


<h2 id="tutors">Organizers</h2>

Afsaneh Shirazi, Alex Ku, Arjun Gopalan, Arsha Nagrani, Blaž Bratanič, Cesar Ilharco, Chris Bregler‎, Christina Liu, Felipe Ferreira, Gabriel Barcik, Gabriel Ilharco, Georg Osang, Jannis Bulian, Jared Frank, Lucas Smaira, Qin Cao, Ricardo Marino, Thomas Leung and Vaiva Imbrasaite.


<h2 id="tutors">Acknowledgements</h2>

We would like to thank Abby Schantz, Abe Ittycheriah, Aliaksei Severyn, Allan Heydon, Aly Grealish, Andrey Vlasov, Arkaitz Zubiaga, Ashwin Kakarla, Chen Sun, Clayton Williams, Cong Yu, Cordelia Schmid, Da-Cheng Juan, Dan Finnie, Dani Valevski, Daniel Rocha, David Chiang, David Price, David Sklar, Devi Krishna, Elena Kochkina, Enrique Alfonseca, Françoise Beaufays, Isabel Kraus-Liang, Isabelle Augenstein, Iulia Turc, Jacob Eisenstein, Jialu Liu, John Cantwell, John Palowitch, Jordan Boyd-Graber, Kenton Lee, Lei Shi, Luís Valente, Maria Voitovich, Mehmet Aktuna, Min Zhang, Mogan Brown, Mohammad Khan, Mor Naaman, Natalia P, Nidhi Hebbar, Pete Aykroyd, Rahul Sukthankar, Richa Dixit, Sol Rosenberg, Steve Pucci, Tania Bedrax-Weiss, Tim Dettmers, Tobias Kaufmann, Tom Boulos, Tu Tsao, Vladimir Chtchetkine, Yair Kurzion, Yifan Xu and Zach Hynes.

      
    </main>
  </body>
</html>
