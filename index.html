<!DOCTYPE html>
<html lang="en-US">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta charset="UTF-8">

<meta property="og:image" content="favicon.png">

<link rel="icon" href="favicon.png" type="image/png">

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Recognizing Multimodal Entailment</title>
<meta name="generator" content="Jekyll v3.9.0">
<meta property="og:title" content="Welcome">
<meta property="og:locale" content="en_US">
<meta name="description" content="Recognizing Multimodal Entailment at ACL-IJCNLP 2021: The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing.">
<meta property="og:description" content="Recognizing Multimodal Entailment at ACL-IJCNLP 2021: The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing.">
<link rel="canonical" href="https://multimodal-entailment.github.io//">
<meta property="og:url" content="https://multimodal-entailment.github.io//">
<meta property="og:site_name" content="Recognizing Multimodal Entailment">
<script type="application/ld+json">
{"description":"Recognizing Multimodal Entailment at ACL-IJCNLP 2021: The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing.","name":"Recognizing Multimodal Entailment","@type":"WebSite","url":"https://multimodal-entailment.github.io//","headline":"Welcome","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=c64178984bc3b4827ae20ab3ebe9e05d036cc516">
  </head>
  <body>

    <header class="page-header" role="banner">
    <h1>Recognizing Multimodal Entailment</h1>
    <h2>ACL-IJCNLP 2021: The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</h2>
    </header>

    <main id="content" class="main-content" role="main">


<div style="display: flex, margin: auto" align="center">  
  <div style="width:64px;" align="center">
    <img src="favicon.png" style="max-width:100%;">
  </div>
  </div>

      <h2 id="welcome">Welcome</h2>

<p>How information is created, shared and consumed has changed rapidly in recent decades, in part thanks to new social platforms and technologies on the web. With ever-larger amounts of unstructured and limited labels, organizing and reconciling information from different sources and modalities is a central challenge in machine learning.
<br><br>
This cutting-edge tutorial aims to introduce the multimodal entailment task, which can be useful for detecting semantic alignments when a single modality alone does not suffice for a whole content understanding. Starting with a brief overview of natural language processing, computer vision, structured data and neural graph learning, we lay the foundations for the multimodal sections to follow. We then discuss recent multimodal learning literature covering visual, audio and language streams, and explore case studies focusing on tasks which require fine-grained understanding of visual and linguistic semantics question answering, veracity and hatred classification. Finally, we introduce a new dataset for recognizing multimodal entailment, exploring it in a hands-on collaborative section. 
<br><br>
Overall, this tutorial gives an overview of multimodal learning, introduces a multimodal entailment dataset, and encourages future research in the topic.
</p>

<h2 id="venue">Venue</h2>

<p>The <em>Recognizing Multimodal Entailment</em> tutorial will be held virtually at <a href="https://2021.aclweb.org/"><em>ACL-IJCNLP 2021: The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</em></a> on Sunday, August 1st, 2021.</p>

<h2 id="outline">Outline</h2>

<table>
   <tbody>
      <tr>
         <td>Section</td>
         <td>Subsection</td>
         <td style="text-align:right;">min</td>
      </tr>
      <tr>
         <td rowspan="2">Introduction
            <br>
         </td>
         <td>The landscape of online content</td>
         <td style="text-align:right;">5</td>
      </tr>
      <tr>
         <td>A case for multimodal entailment inferences</td>
         <td style="text-align:right;">5</td>
      </tr>
      <tr>
         <td rowspan="3">Natural
            <br>
            Language
            <br>
            Processing
         </td>
         <td>From word embeddings to contextualized representations</td>
         <td style="text-align:right;">15</td>
      </tr>
      <tr>
         <td>Fine-tuning pretrained models on downstream tasks</td>
         <td style="text-align:right;">5</td>
      </tr>
      <tr>
         <td>The textual entailment problem</td>
         <td style="text-align:right;">5</td>
      </tr>
      <tr>
         <td rowspan="2">Structured Data
            <br>
         </td>
         <td>Semi-structured and tabular text</td>
         <td style="text-align:right;">5</td>
      </tr>
      <tr>
         <td>Knowledge graphs</td>
         <td style="text-align:right;">5</td>
      </tr>
      <tr>
         <td>Neural Graph Learning</td>
         <td>Leveraging structured signals with Neural Structured Learning</td>
         <td style="text-align:right;">10</td>
      </tr>
      <tr>
         <td>Computer Vision</td>
         <td>Foundations of Computer Vision</td>
         <td style="text-align:right;">20</td>
      </tr>
      <tr>
         <td>Break</td>
         <td>-</td>
         <td style="text-align:right;">10</td>
      </tr>
      <tr>
         <td rowspan="3">Multimodal Learning
            <br><br>
         </td>
         <td>Attention Bottlenecks for Multimodal Fusion: state-of-the-art audio-visual classifications</td>
         <td style="text-align:right;">20</td>
      </tr>
      <tr>
         <td>Self-Supervised Multimodal Versatile Networks:  visual, audio and language streams</td>
         <td style="text-align:right;">20</td>
      </tr>
      <tr>
         <td>Case studies: cross-modal fine-grained reasoning</td>
         <td style="text-align:right;">20</td>
      </tr>
      <tr>
         <td>Break</td>
         <td>-</td>
         <td style="text-align:right;">10</td>
      </tr>
      <tr>
         <td rowspan="2">Multimodal entailment
            <br>
         </td>
         <td>Multimodal models for entailment inferences</td>
         <td style="text-align:right;">20</td>
      </tr>
      <tr>
         <td>Multimodal entailment with Tensorflow on Colab</td>
         <td style="text-align:right;">20</td>
      </tr>
      <tr>
         <td rowspan="2">Final considerations
            <br>
         </td>
         <td>Closing notes</td>
         <td style="text-align:right;">5</td>
      </tr>
      <tr>
         <td>Q&amp;A</td>
         <td style="text-align:right;">10</td>
      </tr>
      <tr>
         <td>Total</td>
         <td>–</td>
         <td style="text-align:right;">210</td>
      </tr>
   </tbody>
</table>


<h2 id="slides">Slides</h2>

Stay tuned ...

<h2 id="reading-list">Reading list</h2>

Stay tuned ...


<h2 id="tutors">Presenters</h2>

<div style="display: flex">
  <div style="width:250px;" align="center">
    <img alt="Afsaneh Shirazi" src="./images/afsaneh_shirazi.jpg" style="max-width:100%;">
    </a><br>
    Afsaneh Shirazi,<br>Google
  </div>
  
  <div style="width:2%">
  </div>
  
  <div style="width:250px;" align="center">
    <img alt="Arjun Gopalan" src="./images/arjun_gopalan.jpg" style="max-width:100%;">
    </a><br>
    Arjun Gopalan,<br>Google Research
  </div>

  <div style="width:2%">
  </div>

  <div style="width:250px;" align="center">
    <img alt="Arsha Nagrani" src="./images/arsha_nagrani.jpg" style="max-width:100%;">
    </a><br>
    Arsha Nagrani,<br>Google Research
  </div>
</div>

<br>

<div style="display: flex">
  <div style="width:250px;" align="center">
    <img alt="Cesar Ilharco" src="./images/cesar_ilharco.jpg" style="max-width:100%;">
    </a><br>
    Cesar Ilharco,<br>Google
  </div>
  
  <div style="width:2%">
  </div>
  
  <div style="width:250px;" align="center">
    <img alt="Christina Liu" src="./images/christina_liu.jpg" style="max-width:100%;">
    </a><br>
    Christina Liu,<br>Google Research
  </div>

  <div style="width:2%">
  </div>

  <div style="width:250px;" align="center">
    <img alt="Gabriel Barcik" src="./images/gabriel_barcik.jpg" style="max-width:100%;">
    </a><br>
    Gabriel Barcik,<br>Google Research
  </div>
</div>

<br>

<div style="display: flex">
  <div style="width:250px;" align="center">
    <img alt="Jannis Bulian" src="./images/jannis_bulian.jpg" style="max-width:100%;">
    </a><br>
    Jannis Bulian,<br>Google Research
  </div>
  
  <div style="width:2%">
  </div>
  
  <div style="width:250px;" align="center">
    <img alt="Jared Frank" src="./images/jared_frank.jpg" style="max-width:100%;">
    </a><br>
    Jared Frank,<br>Google
  </div>

  <div style="width:2%">
  </div>

  <div style="width:250px;" align="center">
    <img alt="Lucas Smaira" src="./images/lucas_smaira.jpg" style="max-width:100%;">
    </a><br>
    Lucas Smaira,<br>DeepMind
  </div>
</div>
<br>

<div style="display: flex">
  <div style="width:250px;" align="center">
    <img alt="Qin Cao" src="./images/qin_cao.jpg" style="max-width:100%;">
    </a><br>
    Qin Cao,<br>Google Research
  </div>
  
  <div style="width:2%">
  </div>
  
  <div style="width:250px;" align="center">
    <img alt="Ricardo Marino" src="./images/ricardo_marino.jpg" style="max-width:100%;">
    </a><br>
    Ricardo Marino,<br>Google
  </div>

  <div style="width:2%">
  </div>

  <div style="width:250px;" align="center">
    <img alt="Roma Patel" src="./images/roma_patel.jpg" style="max-width:100%;">
    </a><br>
    Roma Patel,<br>Brown University
  </div>
</div>


<h2 id="tutors">Organizers</h2>

Afsaneh Shirazi, Alex Ku, Arjun Gopalan, Arsha Nagrani, Blaž Bratanič, Cesar Ilharco, Chris Bregler‎, Christina Liu, Felipe Ferreira, Gabriel Barcik, Gabriel Ilharco, Georg Osang, Jannis Bulian, Jared Frank, Lucas Smaira, Qin Cao, Ricardo Marino, Thomas Leung and Vaiva Imbrasaite.


<h2 id="tutors">Acknowledgements</h2>

We would like to thank Abby Schantz, Abe Ittycheriah, Aliaksei Severyn, Allan Heydon, Aly Grealish, Andrey Vlasov, Arkaitz Zubiaga, Ashwin Kakarla, Chen Sun, Clayton Williams, Cong Yu, Cordelia Schmid, Da-Cheng Juan, Dan Finnie, Dani Valevski, Daniel Rocha, David Chiang, David Price, David Sklar, Devi Krishna, Elena Kochkina, Enrique Alfonseca, Françoise Beaufays, Isabel Kraus-Liang, Isabelle Augenstein, Iulia Turc, Jialu Liu, John Cantwell, John Palowitch, Jordan Boyd-Graber, Kenton Lee, Lei Shi, Luís Valente, Maria Voitovich, Mehmet Aktuna, Min Zhang, Mogan Brown, Mor Naaman, Natalia P, Nidhi Hebbar, Pete Aykroyd, Rahul Sukthankar, Richa Dixit, Sol Rosenberg, Steve Pucci, Tania Bedrax-Weiss, Tim Dettmers, Tobias Kaufmann, Tom Boulos, Tu Tsao, Vladimir Chtchetkine, Yair Kurzion, Yifan Xu and Zach Hynes.

      
    </main>
  </body>
</html>
